{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "xyVSRZqcmOHo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1655454336286,
     "user": {
      "displayName": "和泳毅",
      "userId": "15975522221948227517"
     },
     "user_tz": -480
    },
    "id": "xyVSRZqcmOHo",
    "outputId": "5329e459-9a32-4e3c-a532-2f960c3facb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 17 16:38:36 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 472.19       Driver Version: 472.19       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   51C    P8    12W /  N/A |    337MiB /  6144MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1552    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      3580    C+G   ...n64\\EpicGamesLauncher.exe    N/A      |\n",
      "|    0   N/A  N/A     10920    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     12184    C+G   ...omCursor\\CustomCursor.exe    N/A      |\n",
      "|    0   N/A  N/A     23280    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     23832    C+G   ...s\\Win64\\EpicWebHelper.exe    N/A      |\n",
      "|    0   N/A  N/A     37096    C+G   ...disk\\baidunetdiskhost.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "U2TVubnLlfGj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "executionInfo": {
     "elapsed": 548,
     "status": "error",
     "timestamp": 1655454075887,
     "user": {
      "displayName": "和泳毅",
      "userId": "15975522221948227517"
     },
     "user_tz": -480
    },
    "id": "U2TVubnLlfGj",
    "outputId": "da4aae57-3dea-4256-dd6f-c1440d390c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 1.11.0+cu113\n",
      "dgl.__version__: 0.8.2\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.nn import GraphConv,SAGEConv,MaxPooling\n",
    "import dgl.data\n",
    "from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PPIDataset\n",
    "from dgl import DropEdge,AddSelfLoop,GCNNorm,LaplacianPE\n",
    "from torch_geometric.nn import GCNConv, PairNorm\n",
    "from torch_geometric.utils import dropout_adj\n",
    "from functools import namedtuple\n",
    "from load_graph import simple_dataloader, Load_graph\n",
    "print(f\"torch.__version__: {torch.__version__}\")\n",
    "print(f\"dgl.__version__: {dgl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "da696784",
   "metadata": {
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1655453465831,
     "user": {
      "displayName": "和泳毅",
      "userId": "15975522221948227517"
     },
     "user_tz": -480
    },
    "id": "da696784"
   },
   "outputs": [],
   "source": [
    "def train(g, model, epochs, lr):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.8, patience=3,threshold=0.0001, verbose=True, min_lr=1e-8)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    for e in range(epochs):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that you should only compute the losses of the nodes in the training set.\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de52c8c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1655453468083,
     "user": {
      "displayName": "和泳毅",
      "userId": "15975522221948227517"
     },
     "user_tz": -480
    },
    "id": "de52c8c3",
    "outputId": "3651762f-6be7-4c57-bcf9-496f23aec300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = Load_graph('cora', 'node')\n",
    "# print('Number of categories:', dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6bb023",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14456,
     "status": "ok",
     "timestamp": 1655453821753,
     "user": {
      "displayName": "和泳毅",
      "userId": "15975522221948227517"
     },
     "user_tz": -480
    },
    "id": "fa6bb023",
    "outputId": "d4837eeb-3d84-4628-984a-fa4d25b91219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features\n",
      "{'train_mask': tensor([False, False, False,  ..., False, False, False], device='cuda:0'), 'label': tensor([4, 4, 4,  ..., 4, 3, 3], device='cuda:0'), 'val_mask': tensor([False, False,  True,  ..., False, False, False], device='cuda:0'), 'test_mask': tensor([ True,  True, False,  ..., False, False, False], device='cuda:0'), 'feat': tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0526, 0.0000]],\n",
      "       device='cuda:0')}\n",
      "Edge features\n",
      "{'__orig__': tensor([  298,  9199,  1153,  ..., 10415,  5255,  6356], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "g = dataset[0].to('cuda')\n",
    "print('Node features')\n",
    "print(g.ndata)\n",
    "print('Edge features')\n",
    "print(g.edata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3f3928b2",
   "metadata": {
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1655453529365,
     "user": {
      "displayName": "和泳毅",
      "userId": "15975522221948227517"
     },
     "user_tz": -480
    },
    "id": "3f3928b2"
   },
   "outputs": [],
   "source": [
    "class GCNBlock(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation=None, pair_norm=False, add_self_loops=False, drop_edge=0):\n",
    "        super(GCNBlock, self).__init__()\n",
    "#         self.conv = SAGEConv(in_feats, out_feats,'mean')\n",
    "        self.conv = GraphConv(in_feats, out_feats)\n",
    "        self.dropedge = DropEdge(p=drop_edge)\n",
    "        self.self_loops = AddSelfLoop()\n",
    "        self.norm = PairNorm()\n",
    "        self.activation = activation\n",
    "        self.pair_norm = pair_norm\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.drop_edge = drop_edge\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        if self.drop_edge:\n",
    "            g = self.dropedge(g)\n",
    "            self.add_self_loops = True\n",
    "        if self.add_self_loops:\n",
    "            g = self.self_loops(g)\n",
    "        h = self.conv(g, in_feat)\n",
    "        if self.pair_norm:\n",
    "            h = self.norm(h)\n",
    "        if self.activation:\n",
    "            h = self.activation(h)\n",
    "        return h\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers, activation, dropout=0., \n",
    "                 pair_norm=False, add_self_loops=False, drop_edge=0):\n",
    "        super(GCN, self).__init__()\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input layer\n",
    "        self.layer1 = GCNBlock(in_feats, n_hidden, activation, pair_norm=pair_norm,add_self_loops=add_self_loops, drop_edge=drop_edge)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GCNBlock(n_hidden, n_hidden, activation, pair_norm=pair_norm,add_self_loops=add_self_loops, drop_edge=drop_edge))\n",
    "        # output layer\n",
    "        self.layers.append(GCNBlock(n_hidden, n_classes, activation=None, pair_norm=pair_norm,add_self_loops=add_self_loops,drop_edge=drop_edge))\n",
    "        # self.classify = nn.Linear(n_classes, n_classes)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = self.layer1(g, features)\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if idx > 0 and self.dropout:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h)\n",
    "        if self.dropout:\n",
    "            h = self.dropout(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e3ccb99c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1679,
     "status": "ok",
     "timestamp": 1655453552222,
     "user": {
      "displayName": "和泳毅",
      "userId": "15975522221948227517"
     },
     "user_tz": -480
    },
    "id": "e3ccb99c",
    "outputId": "274aaa0e-3f78-4d5d-c718-74f4738500b4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.946, val acc: 0.246 (best 0.246), test acc: 0.244 (best 0.244)\n",
      "In epoch 5, loss: 0.544, val acc: 0.776 (best 0.778), test acc: 0.794 (best 0.789)\n",
      "In epoch 10, loss: 0.031, val acc: 0.788 (best 0.796), test acc: 0.817 (best 0.828)\n",
      "Epoch 00012: reducing learning rate of group 0 to 2.4000e-02.\n",
      "In epoch 15, loss: 0.003, val acc: 0.784 (best 0.796), test acc: 0.799 (best 0.828)\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.9200e-02.\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.5360e-02.\n",
      "In epoch 20, loss: 0.001, val acc: 0.776 (best 0.796), test acc: 0.797 (best 0.828)\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.2288e-02.\n",
      "In epoch 25, loss: 0.000, val acc: 0.778 (best 0.796), test acc: 0.793 (best 0.828)\n",
      "Epoch 00028: reducing learning rate of group 0 to 9.8304e-03.\n",
      "In epoch 30, loss: 0.000, val acc: 0.778 (best 0.796), test acc: 0.792 (best 0.828)\n",
      "Epoch 00032: reducing learning rate of group 0 to 7.8643e-03.\n",
      "In epoch 35, loss: 0.000, val acc: 0.778 (best 0.796), test acc: 0.792 (best 0.828)\n",
      "Epoch 00036: reducing learning rate of group 0 to 6.2915e-03.\n",
      "Epoch 00040: reducing learning rate of group 0 to 5.0332e-03.\n",
      "In epoch 40, loss: 0.000, val acc: 0.776 (best 0.796), test acc: 0.791 (best 0.828)\n",
      "Epoch 00044: reducing learning rate of group 0 to 4.0265e-03.\n",
      "In epoch 45, loss: 0.000, val acc: 0.776 (best 0.796), test acc: 0.791 (best 0.828)\n",
      "Epoch 00048: reducing learning rate of group 0 to 3.2212e-03.\n"
     ]
    }
   ],
   "source": [
    "g = dataset[0].to('cuda')\n",
    "model = GCN(g.ndata['feat'].shape[1], 512, dataset.num_classes, \n",
    "            n_layers=1,\n",
    "            activation=F.relu, \n",
    "            dropout=0, \n",
    "            pair_norm=False, \n",
    "            add_self_loops=True, \n",
    "            drop_edge=0).to('cuda')\n",
    "train(g, model, epochs=50, lr=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d152c56",
   "metadata": {},
   "source": [
    "ppi train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6436008f",
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1655393001295,
     "user": {
      "displayName": "和泳毅",
      "userId": "15975522221948227517"
     },
     "user_tz": -480
    },
    "id": "6436008f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "def micro_f1_score(logits, labels):\n",
    "    predict = np.where(logits.detach().numpy() >= 0., 1, 0)\n",
    "#     print(labels.numpy(),predict)\n",
    "    return f1_score(labels.numpy(), predict, average='micro')\n",
    "\n",
    "\n",
    "def evaluate(model, g, feats, labels):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        logits = model(g, feats)\n",
    "    return micro_f1_score(logits.cpu() , labels.cpu() )\n",
    "\n",
    "def train(epochs, lr, n_hidden, n_layers, activation, dropout=0., pair_norm=False, add_self_loops=False, drop_edge=0):\n",
    "    train_dataset = PPIDataset(mode='train')\n",
    "    valid_dataset = PPIDataset(mode='valid')\n",
    "    test_dataset = PPIDataset(mode='test')\n",
    "\n",
    "    # data loader\n",
    "    train_loader = GraphDataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    valid_loader = GraphDataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = GraphDataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    g = train_dataset[0].to('cuda')\n",
    "    n_classes = train_dataset.num_labels\n",
    "    n_features = g.ndata['feat'].shape[1]\n",
    "\n",
    "    model = GCN(n_features, \n",
    "                n_hidden, \n",
    "                n_classes,\n",
    "                n_layers=n_layers,\n",
    "                activation=activation, \n",
    "                dropout=dropout, \n",
    "                pair_norm=pair_norm,\n",
    "                add_self_loops=add_self_loops,\n",
    "                drop_edge=drop_edge\n",
    "               ).to('cuda')\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.8, patience=5,threshold=0.0001, verbose=True, min_lr=1e-8)\n",
    "    best_val_f1 = 0\n",
    "    best_test_f1 = 0\n",
    "    for e in range(epochs):\n",
    "        losses = []\n",
    "        train_scores = []\n",
    "        for bg in train_loader:\n",
    "            bg = bg.to('cuda')\n",
    "#             transform1 = AddSelfLoop()\n",
    "#             bg = transform1(bg)\n",
    "            logits = model(bg, bg.ndata['feat'])\n",
    "            labels = bg.ndata['label'].float()\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "            losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        val_scores = [evaluate(model, bg.to('cuda'), bg.to('cuda').ndata['feat'], bg.to('cuda').ndata['label']) for bg in valid_loader]\n",
    "        test_scores = [evaluate(model, bg.to('cuda'), bg.to('cuda').ndata['feat'], bg.to('cuda').ndata['label']) for bg in test_loader]\n",
    "        \n",
    "        val_f1 = np.array(val_scores).mean()\n",
    "        test_f1 = np.array(test_scores).mean()\n",
    "        \n",
    "        if best_val_f1 < val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_test_f1 = test_f1\n",
    "            \n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('In epoch {}, loss: {:.3f}, val f1: {:.3f} (best {:.3f}), test f1: {:.3f} (best {:.3f})'.format(\n",
    "                e, np.array(losses).mean(), val_f1, best_val_f1, test_f1, best_test_f1))\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1433,
   "id": "a22cd18c",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1655387845933,
     "user": {
      "displayName": "和泳毅",
      "userId": "15975522221948227517"
     },
     "user_tz": -480
    },
    "id": "a22cd18c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.608, val f1: 0.498 (best 0.498), test f1: 0.508 (best 0.508)\n",
      "In epoch 5, loss: 0.352, val f1: 0.714 (best 0.714), test f1: 0.736 (best 0.736)\n",
      "In epoch 10, loss: 0.217, val f1: 0.823 (best 0.823), test f1: 0.850 (best 0.850)\n",
      "In epoch 15, loss: 0.141, val f1: 0.884 (best 0.884), test f1: 0.906 (best 0.906)\n",
      "In epoch 20, loss: 0.105, val f1: 0.904 (best 0.904), test f1: 0.926 (best 0.926)\n",
      "In epoch 25, loss: 0.073, val f1: 0.931 (best 0.931), test f1: 0.951 (best 0.951)\n",
      "In epoch 30, loss: 0.059, val f1: 0.939 (best 0.939), test f1: 0.958 (best 0.958)\n",
      "In epoch 35, loss: 0.046, val f1: 0.947 (best 0.947), test f1: 0.965 (best 0.965)\n",
      "In epoch 40, loss: 0.034, val f1: 0.955 (best 0.955), test f1: 0.971 (best 0.971)\n",
      "In epoch 45, loss: 0.029, val f1: 0.958 (best 0.958), test f1: 0.973 (best 0.973)\n",
      "In epoch 50, loss: 0.032, val f1: 0.948 (best 0.958), test f1: 0.964 (best 0.973)\n",
      "Epoch 00052: reducing learning rate of group 0 to 4.0000e-03.\n",
      "In epoch 55, loss: 0.019, val f1: 0.965 (best 0.965), test f1: 0.979 (best 0.979)\n",
      "In epoch 60, loss: 0.015, val f1: 0.967 (best 0.967), test f1: 0.980 (best 0.980)\n",
      "In epoch 65, loss: 0.013, val f1: 0.968 (best 0.968), test f1: 0.981 (best 0.981)\n",
      "In epoch 70, loss: 0.012, val f1: 0.968 (best 0.968), test f1: 0.981 (best 0.981)\n",
      "In epoch 75, loss: 0.014, val f1: 0.967 (best 0.969), test f1: 0.980 (best 0.981)\n",
      "Epoch 00078: reducing learning rate of group 0 to 3.2000e-03.\n",
      "In epoch 80, loss: 0.011, val f1: 0.969 (best 0.969), test f1: 0.981 (best 0.981)\n",
      "In epoch 85, loss: 0.011, val f1: 0.969 (best 0.970), test f1: 0.981 (best 0.982)\n",
      "In epoch 90, loss: 0.009, val f1: 0.970 (best 0.971), test f1: 0.982 (best 0.982)\n",
      "Epoch 00094: reducing learning rate of group 0 to 2.5600e-03.\n",
      "In epoch 95, loss: 0.008, val f1: 0.971 (best 0.971), test f1: 0.983 (best 0.983)\n",
      "In epoch 100, loss: 0.007, val f1: 0.971 (best 0.972), test f1: 0.982 (best 0.983)\n",
      "In epoch 105, loss: 0.007, val f1: 0.972 (best 0.972), test f1: 0.983 (best 0.983)\n",
      "Epoch 00106: reducing learning rate of group 0 to 2.0480e-03.\n",
      "In epoch 110, loss: 0.006, val f1: 0.972 (best 0.972), test f1: 0.983 (best 0.983)\n",
      "In epoch 115, loss: 0.006, val f1: 0.972 (best 0.972), test f1: 0.983 (best 0.983)\n",
      "Epoch 00117: reducing learning rate of group 0 to 1.6384e-03.\n",
      "In epoch 120, loss: 0.006, val f1: 0.972 (best 0.972), test f1: 0.983 (best 0.983)\n",
      "Epoch 00124: reducing learning rate of group 0 to 1.3107e-03.\n",
      "In epoch 125, loss: 0.005, val f1: 0.972 (best 0.972), test f1: 0.983 (best 0.983)\n",
      "Epoch 00130: reducing learning rate of group 0 to 1.0486e-03.\n",
      "In epoch 130, loss: 0.005, val f1: 0.972 (best 0.972), test f1: 0.983 (best 0.983)\n",
      "In epoch 135, loss: 0.005, val f1: 0.972 (best 0.972), test f1: 0.983 (best 0.983)\n",
      "Epoch 00136: reducing learning rate of group 0 to 8.3886e-04.\n",
      "In epoch 140, loss: 0.005, val f1: 0.972 (best 0.972), test f1: 0.983 (best 0.983)\n",
      "Epoch 00142: reducing learning rate of group 0 to 6.7109e-04.\n",
      "In epoch 145, loss: 0.005, val f1: 0.972 (best 0.972), test f1: 0.983 (best 0.984)\n",
      "In epoch 150, loss: 0.005, val f1: 0.972 (best 0.972), test f1: 0.984 (best 0.983)\n",
      "Epoch 00153: reducing learning rate of group 0 to 5.3687e-04.\n",
      "In epoch 155, loss: 0.005, val f1: 0.972 (best 0.972), test f1: 0.984 (best 0.983)\n",
      "Epoch 00159: reducing learning rate of group 0 to 4.2950e-04.\n",
      "In epoch 160, loss: 0.005, val f1: 0.972 (best 0.972), test f1: 0.983 (best 0.984)\n",
      "Epoch 00165: reducing learning rate of group 0 to 3.4360e-04.\n",
      "In epoch 165, loss: 0.005, val f1: 0.972 (best 0.972), test f1: 0.983 (best 0.984)\n",
      "In epoch 170, loss: 0.005, val f1: 0.972 (best 0.972), test f1: 0.984 (best 0.984)\n",
      "Epoch 00171: reducing learning rate of group 0 to 2.7488e-04.\n",
      "In epoch 175, loss: 0.005, val f1: 0.972 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00179: reducing learning rate of group 0 to 2.1990e-04.\n",
      "In epoch 180, loss: 0.005, val f1: 0.972 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00185: reducing learning rate of group 0 to 1.7592e-04.\n",
      "In epoch 185, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 190, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00191: reducing learning rate of group 0 to 1.4074e-04.\n",
      "In epoch 195, loss: 0.005, val f1: 0.972 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00197: reducing learning rate of group 0 to 1.1259e-04.\n",
      "In epoch 200, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00203: reducing learning rate of group 0 to 9.0072e-05.\n",
      "In epoch 205, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00209: reducing learning rate of group 0 to 7.2058e-05.\n",
      "In epoch 210, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00215: reducing learning rate of group 0 to 5.7646e-05.\n",
      "In epoch 215, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 220, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00221: reducing learning rate of group 0 to 4.6117e-05.\n",
      "In epoch 225, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00227: reducing learning rate of group 0 to 3.6893e-05.\n",
      "In epoch 230, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00233: reducing learning rate of group 0 to 2.9515e-05.\n",
      "In epoch 235, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00239: reducing learning rate of group 0 to 2.3612e-05.\n",
      "In epoch 240, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00245: reducing learning rate of group 0 to 1.8889e-05.\n",
      "In epoch 245, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 250, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00251: reducing learning rate of group 0 to 1.5112e-05.\n",
      "In epoch 255, loss: 0.005, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 260, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00263: reducing learning rate of group 0 to 1.2089e-05.\n",
      "In epoch 265, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00269: reducing learning rate of group 0 to 9.6714e-06.\n",
      "In epoch 270, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00275: reducing learning rate of group 0 to 7.7371e-06.\n",
      "In epoch 275, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 280, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00281: reducing learning rate of group 0 to 6.1897e-06.\n",
      "In epoch 285, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00287: reducing learning rate of group 0 to 4.9518e-06.\n",
      "In epoch 290, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00293: reducing learning rate of group 0 to 3.9614e-06.\n",
      "In epoch 295, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00299: reducing learning rate of group 0 to 3.1691e-06.\n",
      "In epoch 300, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00305: reducing learning rate of group 0 to 2.5353e-06.\n",
      "In epoch 305, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 310, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00311: reducing learning rate of group 0 to 2.0282e-06.\n",
      "In epoch 315, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00317: reducing learning rate of group 0 to 1.6226e-06.\n",
      "In epoch 320, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00323: reducing learning rate of group 0 to 1.2981e-06.\n",
      "In epoch 325, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00329: reducing learning rate of group 0 to 1.0385e-06.\n",
      "In epoch 330, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00335: reducing learning rate of group 0 to 8.3077e-07.\n",
      "In epoch 335, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 340, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00341: reducing learning rate of group 0 to 6.6461e-07.\n",
      "In epoch 345, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00347: reducing learning rate of group 0 to 5.3169e-07.\n",
      "In epoch 350, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00353: reducing learning rate of group 0 to 4.2535e-07.\n",
      "In epoch 355, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00359: reducing learning rate of group 0 to 3.4028e-07.\n",
      "In epoch 360, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00365: reducing learning rate of group 0 to 2.7223e-07.\n",
      "In epoch 365, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 370, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00371: reducing learning rate of group 0 to 2.1778e-07.\n",
      "In epoch 375, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00377: reducing learning rate of group 0 to 1.7422e-07.\n",
      "In epoch 380, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00383: reducing learning rate of group 0 to 1.3938e-07.\n",
      "In epoch 385, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00389: reducing learning rate of group 0 to 1.1150e-07.\n",
      "In epoch 390, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00395: reducing learning rate of group 0 to 8.9203e-08.\n",
      "In epoch 395, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 400, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00401: reducing learning rate of group 0 to 7.1362e-08.\n",
      "In epoch 405, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00407: reducing learning rate of group 0 to 5.7090e-08.\n",
      "In epoch 410, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "Epoch 00413: reducing learning rate of group 0 to 4.5672e-08.\n",
      "In epoch 415, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 420, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 425, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 430, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 435, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 440, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 445, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 450, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 455, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 460, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 465, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 470, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 475, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 480, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 485, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 490, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 495, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 500, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 505, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 510, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 515, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 520, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 525, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 530, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 535, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 540, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 545, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 550, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 555, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 560, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 565, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 570, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 575, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 580, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 585, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 590, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n",
      "In epoch 595, loss: 0.004, val f1: 0.973 (best 0.973), test f1: 0.984 (best 0.984)\n"
     ]
    }
   ],
   "source": [
    "train(epochs=600,lr=0.005,n_hidden=256,n_layers=5,\n",
    "      activation=F.tanh, dropout=0., pair_norm=False, \n",
    "      add_self_loops=True, drop_edge=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176e222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "node_class.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
