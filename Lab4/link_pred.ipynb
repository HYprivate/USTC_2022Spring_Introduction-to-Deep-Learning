{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1acd1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 1.11.0+cu113\n",
      "dgl.__version__: 0.8.2\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import dgl.data\n",
    "from dgl.nn import GraphConv,SAGEConv,MaxPooling\n",
    "import dgl.function as fn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from dgl import DropEdge,AddSelfLoop,GCNNorm,LaplacianPE\n",
    "from torch_geometric.nn import PairNorm\n",
    "from load_graph import simple_dataloader, Load_graph\n",
    "from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PPIDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"torch.__version__: {torch.__version__}\")\n",
    "print(f\"dgl.__version__: {dgl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63579b1b",
   "metadata": {},
   "source": [
    "create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8957bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "    \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c5c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNBlock(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation=None, pair_norm=False, add_self_loops=False, drop_edge=0):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        self.conv = SAGEConv(in_feats, out_feats,'mean')\n",
    "        self.dropedge = DropEdge(p=drop_edge)\n",
    "        self.self_loops = AddSelfLoop()\n",
    "        self.norm = PairNorm()\n",
    "        self.activation = activation\n",
    "        self.pair_norm = pair_norm\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.drop_edge = drop_edge\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        if self.drop_edge:\n",
    "            g = self.dropedge(g)\n",
    "            self.add_self_loops = True\n",
    "        if self.add_self_loops:\n",
    "            g = self.self_loops(g)\n",
    "        h = self.conv(g, in_feat)\n",
    "        if self.pair_norm:\n",
    "            h = self.norm(h)\n",
    "        if self.activation:\n",
    "            h = self.activation(h)\n",
    "        return h\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, out_feats, n_layers, activation, dropout=0., \n",
    "                 pair_norm=False, add_self_loops=False, drop_edge=0):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        if dropout:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer1 = GCNBlock(in_feats, n_hidden, activation, pair_norm=pair_norm,add_self_loops=add_self_loops, drop_edge=drop_edge)\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(GCNBlock(n_hidden, n_hidden, activation, pair_norm=pair_norm,add_self_loops=add_self_loops, drop_edge=drop_edge))\n",
    "\n",
    "        self.layers.append(GCNBlock(n_hidden, out_feats, activation=None, pair_norm=pair_norm,add_self_loops=add_self_loops,drop_edge=drop_edge))\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = self.layer1(g, features)\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if idx > 0 and self.dropout:\n",
    "                h = self.dropout(h)\n",
    "            h = layer(g, h)\n",
    "        if self.dropout:\n",
    "            h = self.dropout(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9624166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata['score'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "429a9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, h_feats):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(h_feats * 2, h_feats)\n",
    "        self.W2 = nn.Linear(h_feats, 1)\n",
    "\n",
    "    def apply_edges(self, edges):\n",
    "        h = torch.cat([edges.src['h'], edges.dst['h']], 1)\n",
    "        return {'score': self.W2(F.relu(self.W1(h))).squeeze(1)}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(self.apply_edges)\n",
    "            return g.edata['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c66e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).detach().numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).detach().numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07e40c",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9776634",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Load_graph('cora', 'edge')\n",
    "g = dataset[1]\n",
    "\n",
    "u, v = g.edges()\n",
    "\n",
    "eids = np.arange(g.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "test_size = int(len(eids) * 0.1)\n",
    "train_size = g.number_of_edges() - test_size\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n",
    "train_g = dgl.remove_edges(g, eids[:test_size])\n",
    "\n",
    "train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
    "train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "430e680b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 111.905, test_auc: 0.511, best_auc: 0.511\n",
      "In epoch 5, loss: 4.329, test_auc: 0.541, best_auc: 0.552\n",
      "In epoch 10, loss: 1.246, test_auc: 0.546, best_auc: 0.552\n",
      "In epoch 15, loss: 0.876, test_auc: 0.543, best_auc: 0.552\n",
      "In epoch 20, loss: 0.765, test_auc: 0.545, best_auc: 0.552\n",
      "In epoch 25, loss: 0.716, test_auc: 0.554, best_auc: 0.554\n",
      "In epoch 30, loss: 0.698, test_auc: 0.565, best_auc: 0.565\n",
      "In epoch 35, loss: 0.690, test_auc: 0.575, best_auc: 0.575\n",
      "In epoch 40, loss: 0.686, test_auc: 0.583, best_auc: 0.583\n",
      "In epoch 45, loss: 0.682, test_auc: 0.592, best_auc: 0.592\n",
      "In epoch 50, loss: 0.680, test_auc: 0.600, best_auc: 0.600\n",
      "In epoch 55, loss: 0.678, test_auc: 0.608, best_auc: 0.608\n",
      "In epoch 60, loss: 0.676, test_auc: 0.616, best_auc: 0.616\n",
      "In epoch 65, loss: 0.674, test_auc: 0.624, best_auc: 0.624\n",
      "In epoch 70, loss: 0.672, test_auc: 0.631, best_auc: 0.631\n",
      "In epoch 75, loss: 0.670, test_auc: 0.638, best_auc: 0.638\n",
      "In epoch 80, loss: 0.667, test_auc: 0.646, best_auc: 0.646\n",
      "In epoch 85, loss: 0.665, test_auc: 0.653, best_auc: 0.653\n",
      "In epoch 90, loss: 0.662, test_auc: 0.661, best_auc: 0.661\n",
      "In epoch 95, loss: 0.660, test_auc: 0.668, best_auc: 0.668\n",
      "In epoch 100, loss: 0.656, test_auc: 0.676, best_auc: 0.676\n",
      "In epoch 105, loss: 0.653, test_auc: 0.684, best_auc: 0.684\n",
      "In epoch 110, loss: 0.649, test_auc: 0.693, best_auc: 0.693\n",
      "In epoch 115, loss: 0.645, test_auc: 0.701, best_auc: 0.701\n",
      "In epoch 120, loss: 0.640, test_auc: 0.710, best_auc: 0.710\n",
      "In epoch 125, loss: 0.634, test_auc: 0.720, best_auc: 0.720\n",
      "In epoch 130, loss: 0.628, test_auc: 0.730, best_auc: 0.730\n",
      "In epoch 135, loss: 0.621, test_auc: 0.741, best_auc: 0.741\n",
      "In epoch 140, loss: 0.615, test_auc: 0.748, best_auc: 0.748\n",
      "In epoch 145, loss: 0.608, test_auc: 0.757, best_auc: 0.757\n",
      "In epoch 150, loss: 0.602, test_auc: 0.764, best_auc: 0.764\n",
      "In epoch 155, loss: 0.596, test_auc: 0.772, best_auc: 0.772\n",
      "In epoch 160, loss: 0.590, test_auc: 0.778, best_auc: 0.778\n",
      "In epoch 165, loss: 0.584, test_auc: 0.783, best_auc: 0.783\n",
      "In epoch 170, loss: 0.578, test_auc: 0.787, best_auc: 0.788\n",
      "In epoch 175, loss: 0.574, test_auc: 0.792, best_auc: 0.792\n",
      "In epoch 180, loss: 0.569, test_auc: 0.795, best_auc: 0.795\n",
      "In epoch 185, loss: 0.565, test_auc: 0.798, best_auc: 0.798\n",
      "In epoch 190, loss: 0.561, test_auc: 0.800, best_auc: 0.800\n",
      "In epoch 195, loss: 0.557, test_auc: 0.800, best_auc: 0.801\n",
      "In epoch 200, loss: 0.554, test_auc: 0.802, best_auc: 0.803\n",
      "In epoch 205, loss: 0.553, test_auc: 0.810, best_auc: 0.810\n",
      "In epoch 210, loss: 0.548, test_auc: 0.808, best_auc: 0.810\n",
      "In epoch 215, loss: 0.546, test_auc: 0.805, best_auc: 0.810\n",
      "In epoch 220, loss: 0.543, test_auc: 0.805, best_auc: 0.811\n",
      "In epoch 225, loss: 0.541, test_auc: 0.807, best_auc: 0.812\n",
      "In epoch 230, loss: 0.538, test_auc: 0.809, best_auc: 0.813\n",
      "In epoch 235, loss: 0.536, test_auc: 0.810, best_auc: 0.813\n",
      "In epoch 240, loss: 0.534, test_auc: 0.811, best_auc: 0.814\n",
      "In epoch 245, loss: 0.531, test_auc: 0.814, best_auc: 0.814\n",
      "In epoch 250, loss: 0.529, test_auc: 0.816, best_auc: 0.816\n",
      "In epoch 255, loss: 0.527, test_auc: 0.811, best_auc: 0.819\n",
      "In epoch 260, loss: 0.525, test_auc: 0.814, best_auc: 0.819\n",
      "In epoch 265, loss: 0.523, test_auc: 0.819, best_auc: 0.820\n",
      "In epoch 270, loss: 0.521, test_auc: 0.820, best_auc: 0.820\n",
      "In epoch 275, loss: 0.519, test_auc: 0.818, best_auc: 0.821\n",
      "In epoch 280, loss: 0.517, test_auc: 0.814, best_auc: 0.822\n",
      "In epoch 285, loss: 0.515, test_auc: 0.812, best_auc: 0.822\n",
      "In epoch 290, loss: 0.513, test_auc: 0.815, best_auc: 0.823\n",
      "In epoch 295, loss: 0.511, test_auc: 0.818, best_auc: 0.823\n",
      "In epoch 300, loss: 0.509, test_auc: 0.822, best_auc: 0.824\n",
      "In epoch 305, loss: 0.508, test_auc: 0.823, best_auc: 0.824\n",
      "In epoch 310, loss: 0.505, test_auc: 0.823, best_auc: 0.824\n",
      "In epoch 315, loss: 0.503, test_auc: 0.820, best_auc: 0.824\n",
      "In epoch 320, loss: 0.502, test_auc: 0.816, best_auc: 0.824\n",
      "In epoch 325, loss: 0.500, test_auc: 0.819, best_auc: 0.824\n",
      "In epoch 330, loss: 0.499, test_auc: 0.825, best_auc: 0.825\n",
      "In epoch 335, loss: 0.496, test_auc: 0.818, best_auc: 0.826\n",
      "In epoch 340, loss: 0.495, test_auc: 0.814, best_auc: 0.826\n",
      "In epoch 345, loss: 0.492, test_auc: 0.821, best_auc: 0.826\n",
      "In epoch 350, loss: 0.492, test_auc: 0.827, best_auc: 0.827\n",
      "In epoch 355, loss: 0.489, test_auc: 0.825, best_auc: 0.827\n",
      "In epoch 360, loss: 0.487, test_auc: 0.818, best_auc: 0.827\n",
      "In epoch 365, loss: 0.487, test_auc: 0.814, best_auc: 0.827\n",
      "In epoch 370, loss: 0.485, test_auc: 0.815, best_auc: 0.827\n",
      "In epoch 375, loss: 0.482, test_auc: 0.819, best_auc: 0.827\n",
      "In epoch 380, loss: 0.481, test_auc: 0.825, best_auc: 0.827\n",
      "In epoch 385, loss: 0.480, test_auc: 0.827, best_auc: 0.827\n",
      "In epoch 390, loss: 0.477, test_auc: 0.823, best_auc: 0.827\n",
      "In epoch 395, loss: 0.477, test_auc: 0.815, best_auc: 0.827\n",
      "In epoch 400, loss: 0.475, test_auc: 0.814, best_auc: 0.827\n",
      "In epoch 405, loss: 0.472, test_auc: 0.821, best_auc: 0.827\n",
      "In epoch 410, loss: 0.472, test_auc: 0.827, best_auc: 0.827\n",
      "In epoch 415, loss: 0.470, test_auc: 0.825, best_auc: 0.827\n",
      "In epoch 420, loss: 0.468, test_auc: 0.818, best_auc: 0.827\n",
      "In epoch 425, loss: 0.467, test_auc: 0.814, best_auc: 0.827\n",
      "In epoch 430, loss: 0.465, test_auc: 0.820, best_auc: 0.827\n",
      "In epoch 435, loss: 0.464, test_auc: 0.827, best_auc: 0.827\n",
      "In epoch 440, loss: 0.462, test_auc: 0.824, best_auc: 0.827\n",
      "In epoch 445, loss: 0.461, test_auc: 0.814, best_auc: 0.827\n",
      "In epoch 450, loss: 0.459, test_auc: 0.815, best_auc: 0.827\n",
      "In epoch 455, loss: 0.457, test_auc: 0.824, best_auc: 0.827\n",
      "In epoch 460, loss: 0.456, test_auc: 0.826, best_auc: 0.827\n",
      "In epoch 465, loss: 0.454, test_auc: 0.817, best_auc: 0.827\n",
      "In epoch 470, loss: 0.453, test_auc: 0.814, best_auc: 0.827\n",
      "In epoch 475, loss: 0.451, test_auc: 0.824, best_auc: 0.827\n",
      "In epoch 480, loss: 0.449, test_auc: 0.824, best_auc: 0.827\n",
      "In epoch 485, loss: 0.449, test_auc: 0.811, best_auc: 0.827\n",
      "In epoch 490, loss: 0.446, test_auc: 0.822, best_auc: 0.828\n",
      "In epoch 495, loss: 0.445, test_auc: 0.826, best_auc: 0.828\n"
     ]
    }
   ],
   "source": [
    "model = GraphSAGE(train_g.ndata['feat'].shape[1],  n_hidden=512, out_feats=128, n_layers=3, activation=F.relu, dropout=0., \n",
    "                 pair_norm=False, add_self_loops=True, drop_edge=0.)\n",
    "\n",
    "# pred = MLPPredictor(16)\n",
    "pred = DotPredictor()\n",
    "\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.0002)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.1, patience=5,threshold=0.0001, verbose=True, min_lr=1e-8)\n",
    "all_logits = []\n",
    "best_auc = 0.\n",
    "for e in range(200):\n",
    "    # forward\n",
    "    h = model(train_g, train_g.ndata['feat'])\n",
    "    train_pos_score = pred(train_pos_g, h)\n",
    "    train_neg_score = pred(train_neg_g, h)\n",
    "    loss = compute_loss(train_pos_score, train_neg_score)\n",
    "    \n",
    "    test_pos_score = pred(test_pos_g, h)\n",
    "    test_neg_score = pred(test_neg_g, h)\n",
    "    test_auc = compute_auc(test_pos_score, test_neg_score)\n",
    "    \n",
    "    if best_auc < test_auc:\n",
    "        best_auc = test_auc\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        print('In epoch {}, loss: {:.3f}, test_auc: {:.3f}, best_auc: {:.3f}'.format(e, loss,  test_auc, best_auc))\n",
    "        \n",
    "#     if scheduler is not None:\n",
    "#         scheduler.step(test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a12116",
   "metadata": {},
   "source": [
    "ppi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d03c2e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PPIDataset(mode='train')\n",
    "data = []\n",
    "for i in range(len(dataset)):\n",
    "    g = dataset[i]\n",
    "    u, v = g.edges()\n",
    "    eids = np.arange(g.number_of_edges())\n",
    "    eids = np.random.permutation(eids)\n",
    "    test_size = int(len(eids) * 0.1)\n",
    "    train_size = g.number_of_edges() - test_size\n",
    "    test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "    train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "    adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "    adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "    neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "    neg_eids = np.random.choice(len(neg_u), g.number_of_edges())\n",
    "    test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "    train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n",
    "    train_g = dgl.remove_edges(g, eids[:test_size])\n",
    "\n",
    "    train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\n",
    "    train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n",
    "\n",
    "    test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\n",
    "    test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n",
    "    \n",
    "    data.append([train_g, train_pos_g, train_neg_g, test_pos_g, test_neg_g])\n",
    "    \n",
    "loader = simple_dataloader(dataset=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e88b8f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.674, test_auc: 0.645, best_auc: 0.645\n",
      "In epoch 5, loss: 0.580, test_auc: 0.782, best_auc: 0.782\n",
      "In epoch 10, loss: 0.558, test_auc: 0.806, best_auc: 0.806\n",
      "In epoch 15, loss: 0.549, test_auc: 0.816, best_auc: 0.816\n",
      "In epoch 20, loss: 0.544, test_auc: 0.822, best_auc: 0.822\n",
      "In epoch 25, loss: 0.540, test_auc: 0.825, best_auc: 0.825\n",
      "In epoch 30, loss: 0.539, test_auc: 0.827, best_auc: 0.827\n",
      "In epoch 35, loss: 0.536, test_auc: 0.829, best_auc: 0.829\n",
      "In epoch 40, loss: 0.535, test_auc: 0.830, best_auc: 0.830\n",
      "In epoch 45, loss: 0.534, test_auc: 0.830, best_auc: 0.830\n",
      "In epoch 50, loss: 0.533, test_auc: 0.831, best_auc: 0.831\n",
      "In epoch 55, loss: 0.532, test_auc: 0.832, best_auc: 0.832\n",
      "In epoch 60, loss: 0.531, test_auc: 0.833, best_auc: 0.833\n",
      "In epoch 65, loss: 0.530, test_auc: 0.833, best_auc: 0.834\n",
      "In epoch 70, loss: 0.531, test_auc: 0.832, best_auc: 0.834\n",
      "In epoch 75, loss: 0.529, test_auc: 0.834, best_auc: 0.835\n",
      "In epoch 80, loss: 0.528, test_auc: 0.835, best_auc: 0.835\n",
      "In epoch 85, loss: 0.528, test_auc: 0.836, best_auc: 0.837\n",
      "In epoch 90, loss: 0.528, test_auc: 0.836, best_auc: 0.837\n",
      "In epoch 95, loss: 0.528, test_auc: 0.835, best_auc: 0.837\n",
      "In epoch 100, loss: 0.528, test_auc: 0.836, best_auc: 0.837\n",
      "In epoch 105, loss: 0.526, test_auc: 0.838, best_auc: 0.838\n",
      "In epoch 110, loss: 0.526, test_auc: 0.838, best_auc: 0.839\n",
      "In epoch 115, loss: 0.526, test_auc: 0.840, best_auc: 0.840\n",
      "In epoch 120, loss: 0.526, test_auc: 0.839, best_auc: 0.840\n",
      "In epoch 125, loss: 0.525, test_auc: 0.840, best_auc: 0.840\n",
      "In epoch 130, loss: 0.524, test_auc: 0.840, best_auc: 0.840\n",
      "In epoch 135, loss: 0.524, test_auc: 0.840, best_auc: 0.840\n",
      "In epoch 140, loss: 0.524, test_auc: 0.841, best_auc: 0.841\n",
      "In epoch 145, loss: 0.523, test_auc: 0.840, best_auc: 0.842\n",
      "In epoch 150, loss: 0.523, test_auc: 0.841, best_auc: 0.842\n",
      "In epoch 155, loss: 0.524, test_auc: 0.840, best_auc: 0.842\n",
      "In epoch 160, loss: 0.524, test_auc: 0.840, best_auc: 0.842\n",
      "In epoch 165, loss: 0.523, test_auc: 0.842, best_auc: 0.842\n",
      "In epoch 170, loss: 0.523, test_auc: 0.842, best_auc: 0.842\n",
      "In epoch 175, loss: 0.523, test_auc: 0.842, best_auc: 0.842\n",
      "In epoch 180, loss: 0.522, test_auc: 0.843, best_auc: 0.843\n",
      "In epoch 185, loss: 0.522, test_auc: 0.841, best_auc: 0.843\n",
      "In epoch 190, loss: 0.522, test_auc: 0.842, best_auc: 0.843\n",
      "In epoch 195, loss: 0.521, test_auc: 0.844, best_auc: 0.844\n",
      "In epoch 200, loss: 0.522, test_auc: 0.843, best_auc: 0.844\n",
      "In epoch 205, loss: 0.522, test_auc: 0.843, best_auc: 0.844\n",
      "In epoch 210, loss: 0.522, test_auc: 0.843, best_auc: 0.844\n",
      "In epoch 215, loss: 0.521, test_auc: 0.844, best_auc: 0.844\n",
      "In epoch 220, loss: 0.521, test_auc: 0.843, best_auc: 0.844\n",
      "In epoch 225, loss: 0.521, test_auc: 0.843, best_auc: 0.844\n",
      "In epoch 230, loss: 0.522, test_auc: 0.842, best_auc: 0.844\n",
      "In epoch 235, loss: 0.521, test_auc: 0.843, best_auc: 0.845\n",
      "In epoch 240, loss: 0.521, test_auc: 0.844, best_auc: 0.845\n",
      "In epoch 245, loss: 0.520, test_auc: 0.844, best_auc: 0.845\n",
      "In epoch 250, loss: 0.520, test_auc: 0.845, best_auc: 0.845\n",
      "In epoch 255, loss: 0.520, test_auc: 0.844, best_auc: 0.845\n",
      "In epoch 260, loss: 0.520, test_auc: 0.844, best_auc: 0.845\n",
      "In epoch 265, loss: 0.520, test_auc: 0.844, best_auc: 0.845\n",
      "In epoch 270, loss: 0.521, test_auc: 0.842, best_auc: 0.845\n",
      "In epoch 275, loss: 0.521, test_auc: 0.844, best_auc: 0.845\n",
      "In epoch 280, loss: 0.520, test_auc: 0.845, best_auc: 0.845\n",
      "In epoch 285, loss: 0.519, test_auc: 0.844, best_auc: 0.845\n",
      "In epoch 290, loss: 0.519, test_auc: 0.844, best_auc: 0.845\n",
      "In epoch 295, loss: 0.519, test_auc: 0.844, best_auc: 0.845\n",
      "In epoch 300, loss: 0.520, test_auc: 0.844, best_auc: 0.845\n",
      "In epoch 305, loss: 0.520, test_auc: 0.844, best_auc: 0.845\n",
      "In epoch 310, loss: 0.521, test_auc: 0.843, best_auc: 0.845\n",
      "In epoch 315, loss: 0.520, test_auc: 0.845, best_auc: 0.845\n",
      "In epoch 320, loss: 0.519, test_auc: 0.845, best_auc: 0.845\n",
      "In epoch 325, loss: 0.519, test_auc: 0.845, best_auc: 0.845\n",
      "In epoch 330, loss: 0.519, test_auc: 0.845, best_auc: 0.845\n",
      "In epoch 335, loss: 0.519, test_auc: 0.844, best_auc: 0.845\n",
      "In epoch 340, loss: 0.519, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 345, loss: 0.521, test_auc: 0.843, best_auc: 0.846\n",
      "In epoch 350, loss: 0.520, test_auc: 0.843, best_auc: 0.846\n",
      "In epoch 355, loss: 0.519, test_auc: 0.844, best_auc: 0.846\n",
      "In epoch 360, loss: 0.519, test_auc: 0.843, best_auc: 0.846\n",
      "In epoch 365, loss: 0.519, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 370, loss: 0.519, test_auc: 0.844, best_auc: 0.846\n",
      "In epoch 375, loss: 0.519, test_auc: 0.844, best_auc: 0.846\n",
      "In epoch 380, loss: 0.519, test_auc: 0.844, best_auc: 0.846\n",
      "In epoch 385, loss: 0.520, test_auc: 0.843, best_auc: 0.846\n",
      "In epoch 390, loss: 0.520, test_auc: 0.843, best_auc: 0.846\n",
      "In epoch 395, loss: 0.520, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 400, loss: 0.519, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 405, loss: 0.519, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 410, loss: 0.518, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 415, loss: 0.518, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 420, loss: 0.518, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 425, loss: 0.518, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 430, loss: 0.518, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 435, loss: 0.518, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 440, loss: 0.519, test_auc: 0.844, best_auc: 0.846\n",
      "In epoch 445, loss: 0.519, test_auc: 0.844, best_auc: 0.846\n",
      "In epoch 450, loss: 0.520, test_auc: 0.843, best_auc: 0.846\n",
      "In epoch 455, loss: 0.519, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 460, loss: 0.519, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 465, loss: 0.518, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 470, loss: 0.518, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 475, loss: 0.519, test_auc: 0.843, best_auc: 0.846\n",
      "In epoch 480, loss: 0.519, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 485, loss: 0.519, test_auc: 0.844, best_auc: 0.846\n",
      "In epoch 490, loss: 0.518, test_auc: 0.845, best_auc: 0.846\n",
      "In epoch 495, loss: 0.518, test_auc: 0.845, best_auc: 0.846\n"
     ]
    }
   ],
   "source": [
    "data_dataset = PPIDataset(mode='train')\n",
    "model = GraphSAGE(data_dataset[0].ndata['feat'].shape[1],  n_hidden=512, out_feats=128, n_layers=3, activation=F.relu, dropout=0., \n",
    "                 pair_norm=True, add_self_loops=True, drop_edge=0.)\n",
    "\n",
    "# pred = MLPPredictor(16)\n",
    "pred = DotPredictor()\n",
    "\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.1, patience=5,threshold=0.0001, verbose=True, min_lr=1e-8)\n",
    "all_logits = []\n",
    "best_auc = 0.\n",
    "for e in range(500):\n",
    "    \n",
    "    # forward\n",
    "    losses=[]\n",
    "    test_aucs=[]\n",
    "    for train_g, train_pos_g, train_neg_g, test_pos_g, test_neg_g in loader:\n",
    "        h = model(train_g, train_g.ndata['feat'])\n",
    "        train_pos_score = pred(train_pos_g, h)\n",
    "        train_neg_score = pred(train_neg_g, h)\n",
    "        loss = compute_loss(train_pos_score, train_neg_score)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        test_pos_score = pred(test_pos_g, h)\n",
    "        test_neg_score = pred(test_neg_g, h)\n",
    "        test_auc = compute_auc(test_pos_score, test_neg_score)\n",
    "        test_aucs.append(test_auc)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    test_auc = np.array(test_aucs).mean()\n",
    "    loss = np.array(losses).mean()\n",
    "    if best_auc < test_auc:\n",
    "        best_auc = test_auc\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        print('In epoch {}, loss: {:.3f}, test_auc: {:.3f}, best_auc: {:.3f}'.format(e, loss,  test_auc, best_auc))\n",
    "        \n",
    "#     if scheduler is not None:\n",
    "#         scheduler.step(test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f10fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
